#!/usr/bin/env python3
"""
æµ‹è¯•çˆ¬è™«è„šæœ¬
å¿«é€Ÿæµ‹è¯•çˆ¬è™«åŠŸèƒ½ï¼Œåªçˆ¬å–å°‘é‡æ–‡ç« 
"""

import sys
import os
import logging
from datetime import datetime

# æ·»åŠ å½“å‰ç›®å½•åˆ°Pythonè·¯å¾„
sys.path.append(os.path.dirname(os.path.abspath(__file__)))

from crawlers.news_crawler import NewsCrawler
from crawlers.tech_crawler import TechCrawler
from crawlers.academic_crawler import AcademicCrawler
from crawlers.manufacturer_crawler import ManufacturerCrawler
from database import Database
from config import Config

# é…ç½®æ—¥å¿—
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s'
)

def test_news_crawler():
    """æµ‹è¯•æ–°é—»çˆ¬è™«"""
    print("ğŸ“° æµ‹è¯•æ–°é—»çˆ¬è™«...")
    try:
        crawler = NewsCrawler()
        articles = crawler.crawl_news_sites(limit=5)  # åªçˆ¬å–5ç¯‡æ–‡ç« 
        print(f"âœ… æ–°é—»çˆ¬è™«æˆåŠŸï¼Œè·å– {len(articles)} ç¯‡æ–‡ç« ")
        return articles
    except Exception as e:
        print(f"âŒ æ–°é—»çˆ¬è™«å¤±è´¥: {e}")
        return []

def test_tech_crawler():
    """æµ‹è¯•æŠ€æœ¯æ–‡ç« çˆ¬è™«"""
    print("ğŸ’» æµ‹è¯•æŠ€æœ¯æ–‡ç« çˆ¬è™«...")
    try:
        crawler = TechCrawler()
        articles = crawler.crawl_tech_sites(limit=3)  # åªçˆ¬å–3ç¯‡æ–‡ç« 
        print(f"âœ… æŠ€æœ¯æ–‡ç« çˆ¬è™«æˆåŠŸï¼Œè·å– {len(articles)} ç¯‡æ–‡ç« ")
        return articles
    except Exception as e:
        print(f"âŒ æŠ€æœ¯æ–‡ç« çˆ¬è™«å¤±è´¥: {e}")
        return []

def test_academic_crawler():
    """æµ‹è¯•å­¦æœ¯è®ºæ–‡çˆ¬è™«"""
    print("ğŸ“š æµ‹è¯•å­¦æœ¯è®ºæ–‡çˆ¬è™«...")
    try:
        crawler = AcademicCrawler()
        articles = crawler.crawl_academic_sites(limit=2)  # åªçˆ¬å–2ç¯‡æ–‡ç« 
        print(f"âœ… å­¦æœ¯è®ºæ–‡çˆ¬è™«æˆåŠŸï¼Œè·å– {len(articles)} ç¯‡æ–‡ç« ")
        return articles
    except Exception as e:
        print(f"âŒ å­¦æœ¯è®ºæ–‡çˆ¬è™«å¤±è´¥: {e}")
        return []

def test_manufacturer_crawler():
    """æµ‹è¯•æ‰‹æœºå‚å•†çˆ¬è™«"""
    print("ğŸ“± æµ‹è¯•æ‰‹æœºå‚å•†çˆ¬è™«...")
    try:
        crawler = ManufacturerCrawler()
        articles = crawler.crawl_manufacturer_sites(limit=5)  # åªçˆ¬å–5ç¯‡æ–‡ç« 
        print(f"âœ… æ‰‹æœºå‚å•†çˆ¬è™«æˆåŠŸï¼Œè·å– {len(articles)} ç¯‡æ–‡ç« ")
        return articles
    except Exception as e:
        print(f"âŒ æ‰‹æœºå‚å•†çˆ¬è™«å¤±è´¥: {e}")
        return []

def test_tech_company_crawler():
    """æµ‹è¯•æŠ€æœ¯å…¬å¸çˆ¬è™«"""
    print("ğŸ¢ æµ‹è¯•æŠ€æœ¯å…¬å¸çˆ¬è™«...")
    try:
        crawler = ManufacturerCrawler()
        articles = crawler.crawl_tech_company_sites(limit=5)  # åªçˆ¬å–5ç¯‡æ–‡ç« 
        print(f"âœ… æŠ€æœ¯å…¬å¸çˆ¬è™«æˆåŠŸï¼Œè·å– {len(articles)} ç¯‡æ–‡ç« ")
        return articles
    except Exception as e:
        print(f"âŒ æŠ€æœ¯å…¬å¸çˆ¬è™«å¤±è´¥: {e}")
        return []

def main():
    print("=" * 60)
    print("ğŸ§ª è“ç‰™æŠ€æœ¯æ–‡ç« çˆ¬è™«æµ‹è¯•")
    print("=" * 60)
    print(f"å¼€å§‹æ—¶é—´: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
    print()
    
    try:
        # åˆå§‹åŒ–æ•°æ®åº“
        print("ğŸ“Š åˆå§‹åŒ–æ•°æ®åº“...")
        db = Database()
        db.init_database()
        print("âœ… æ•°æ®åº“åˆå§‹åŒ–å®Œæˆ")
        
        all_articles = []
        
        # æµ‹è¯•å„ç§çˆ¬è™«
        print("\nğŸ•·ï¸ å¼€å§‹æµ‹è¯•çˆ¬è™«...")
        
        # æµ‹è¯•æ–°é—»çˆ¬è™«
        news_articles = test_news_crawler()
        all_articles.extend(news_articles)
        
        # æµ‹è¯•æŠ€æœ¯æ–‡ç« çˆ¬è™«
        tech_articles = test_tech_crawler()
        all_articles.extend(tech_articles)
        
        # æµ‹è¯•å­¦æœ¯è®ºæ–‡çˆ¬è™«
        academic_articles = test_academic_crawler()
        all_articles.extend(academic_articles)
        
        # æµ‹è¯•æ‰‹æœºå‚å•†çˆ¬è™«
        manufacturer_articles = test_manufacturer_crawler()
        all_articles.extend(manufacturer_articles)
        
        # æµ‹è¯•æŠ€æœ¯å…¬å¸çˆ¬è™«
        tech_company_articles = test_tech_company_crawler()
        all_articles.extend(tech_company_articles)
        
        # ä¿å­˜åˆ°æ•°æ®åº“
        print(f"\nğŸ’¾ ä¿å­˜æ–‡ç« åˆ°æ•°æ®åº“...")
        saved_count = 0
        for article in all_articles:
            try:
                db.insert_article(article)
                saved_count += 1
            except Exception as e:
                print(f"âš ï¸ ä¿å­˜æ–‡ç« å¤±è´¥: {e}")
        
        print(f"âœ… æˆåŠŸä¿å­˜ {saved_count} ç¯‡æ–‡ç« åˆ°æ•°æ®åº“")
        
        # æ˜¾ç¤ºç»Ÿè®¡ä¿¡æ¯
        print("\nğŸ“ˆ æµ‹è¯•ç»“æœç»Ÿè®¡:")
        print(f"   æ€»çˆ¬å–æ–‡ç« : {len(all_articles)} ç¯‡")
        print(f"   æˆåŠŸä¿å­˜: {saved_count} ç¯‡")
        
        if all_articles:
            # æŒ‰æ¥æºç±»å‹ç»Ÿè®¡
            source_counts = {}
            for article in all_articles:
                source_type = article.get('source_type', 'unknown')
                source_counts[source_type] = source_counts.get(source_type, 0) + 1
            
            print("   æ¥æºåˆ†å¸ƒ:")
            for source_type, count in source_counts.items():
                print(f"     - {source_type}: {count} ç¯‡")
        
        print()
        print("ğŸŒ æŸ¥çœ‹ç»“æœ:")
        print("   1. å¯åŠ¨WebæœåŠ¡å™¨: python main.py")
        print("   2. è®¿é—®: http://localhost:5000")
        
    except Exception as e:
        print(f"âŒ æµ‹è¯•è¿‡ç¨‹ä¸­å‡ºç°é”™è¯¯: {e}")
        logging.error(f"æµ‹è¯•å¤±è´¥: {e}")
        return 1
    
    print()
    print("=" * 60)
    print("ğŸ‰ æµ‹è¯•å®Œæˆï¼")
    print("=" * 60)
    return 0

if __name__ == "__main__":
    exit(main()) 